data:
  _target_: spectral_unions.data.datamodule.MyDataModule

  independent_augmentation: true
  introduce_input_evals_noise: false
  min_union_prior: true
  name: PARTIAL_DATASET_V2
  relative_area: true
  train_datasplit_folder: datasplit_multishape

  datasets:
    train:
      _target_: spectral_unions.data.partial_dataset_v2.PartialDatasetV2
#      _target_: spectral_unions.data.random_augmented_dataset.RandomAugmentedDataset
      dataset_name: "${nn.data.name}"
      boundary_conditions: "${global.boundary_conditions}"
      union_num_eigenvalues: "${global.union_num_eigenvalues}"
      part_num_eigenvalues: "${global.part_num_eigenvalues}"


    val:
      - _target_: spectral_unions.data.augmented_dataset.PartialAugmentedDataset

#    test:
#      - _target_: spectral_unions.data.dataset.MyDataset

  gpus: ${train.trainer.gpus}

  num_workers:
    train: 6
    val: 3
#    test: 4

  batch_size:
    train: 32
    val: 16
#    test: 16

  # example
#  val_percentage: 0.1

preprocessing:
  evals_transform: offset

module:
  _target_: spectral_unions.pl_modules.pl_module.UnionTransformerV6

  decoder_dim_feedforward: 32
  decoder_dropout: 0.1
  decoder_nhead: 8
  decoder_nlayers: 3
  encoder_dim_feedforward: 64
  encoder_dropout: 0.1
  encoder_nhead: 8
  encoder_nlayers: 6
  evals_embedding_dim: 32
  last_relu: false

  optimizer:
    #  Adam-oriented deep learning
    _target_: torch.optim.Adam
    #  These are all default parameters for the Adam optimizer
    lr:  0.0002
#    betas: [ 0.9, 0.999 ]
#    eps: 1e-08
    weight_decay: 1e-5

  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    T_0: 10
    T_mult: 2
    eta_min: 0 # min value for the lr
    last_epoch: -1
    verbose: true

loss:
  evals:
    alpha: 1.0
    fun: mse
    idemp_losses: false
    loss_between_encodings: false
    params: null
